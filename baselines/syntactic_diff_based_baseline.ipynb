{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, json, sqlite3, re, difflib, numpy as np, cPickle as cp, glob, random, itertools\n",
    "from collections import OrderedDict\n",
    "import subprocess32 as subprocess\n",
    "from functools import partial\n",
    "np.random.seed(1307)\n",
    "sys.path.append('..')\n",
    "from pycparser import parse_file, c_ast\n",
    "\n",
    "from util.helpers import get_rev_dict, remove_non_ascii, make_dir_if_not_exists as mkdir\n",
    "from util.helpers import get_curr_time_string, tokens_to_source, clang_format_from_source as cf\n",
    "from util.helpers import isolate_line, fetch_line, extract_line_number, get_lines, recompose_program\n",
    "from util.ast_helpers import get_subtree_list, get_linearized_ast, get_ast\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pprint import pprint\n",
    "from util.c_tokenizer import C_Tokenizer\n",
    "tokenize = C_Tokenizer().tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_path = '../data/dataset.db'\n",
    "with sqlite3.connect(db_path) as conn:\n",
    "    c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load eval set\n",
    "eval_set = np.load(os.path.join('../data/', 'eval_set.npy')).item()\n",
    "\n",
    "eval_dict = {}\n",
    "for problem_id in eval_set:\n",
    "    for program_id, row in eval_set[problem_id].items():\n",
    "        eval_dict[program_id] = row\n",
    "eval_set_program_ids = eval_dict.keys()\n",
    "\n",
    "TCNN_correct_classifications = np.load('../data/TCNN_correct_classifications.npy').item()\n",
    "test_wise_faulty_lines = np.load(os.path.join('../data/', 'test_wise_faulty_lines.npy')).item()\n",
    "\n",
    "prog_faulty_lines = {}\n",
    "for program_id in test_wise_faulty_lines:\n",
    "    if program_id not in prog_faulty_lines: prog_faulty_lines[program_id] = set()\n",
    "    for test_id in test_wise_faulty_lines[program_id]:\n",
    "        prog_faulty_lines[program_id].update(test_wise_faulty_lines[program_id][test_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_id_map(ast, program_id=None):\n",
    "    '''shuffles ids before assigning them indices using \n",
    "    program_id as randomness seed if program_id is not None'''\n",
    "    \n",
    "    ids = []\n",
    "    for subtree, coord in ast:\n",
    "        for node in subtree:\n",
    "            if '_<id>_' in node and '@' in node:\n",
    "                org_id = node.split('_<id>_')[1].split('@')[0]\n",
    "                if org_id not in ids:\n",
    "                    ids.append(org_id)\n",
    "                    \n",
    "    if program_id is not None:\n",
    "        random.seed(program_id)\n",
    "        random.shuffle(ids)\n",
    "\n",
    "    id_map = {}\n",
    "    for id_ in ids:\n",
    "        id_map[id_] = len(id_map)\n",
    "    return id_map\n",
    "\n",
    "def normalize_ids(ast, id_map):\n",
    "    new_ast = []\n",
    "    for subtree, coord in ast:\n",
    "        new_subtree = []\n",
    "        for node in subtree:\n",
    "            if '_<id>_' in node and '@' in node:\n",
    "                org_id = node.split('_<id>_')[1].split('@')[0]\n",
    "                new_subtree.append(node.replace('_<id>_' + org_id + '@', '_<id>_' + str(id_map[org_id]) + '@'))\n",
    "            else:\n",
    "                new_subtree.append(node)\n",
    "        assert len(new_subtree) == len(subtree)\n",
    "        new_ast.append((new_subtree, coord))\n",
    "    return new_ast\n",
    "\n",
    "def size_n_vocab_match(subtree_list_ast):\n",
    "    global tl_dict, max_subtrees_per_program, max_nodes_per_subtree\n",
    "    \n",
    "    if len(subtree_list_ast) > max_subtrees_per_program:\n",
    "        return False\n",
    "    \n",
    "    vec_ast = []\n",
    "    \n",
    "    for subtree, coord in subtree_list_ast:\n",
    "        vec_subtree = []\n",
    "        for token in subtree:\n",
    "            try:\n",
    "                vec_subtree.append(tl_dict[token])\n",
    "            except KeyError:\n",
    "                return None\n",
    "\n",
    "        if len(vec_subtree) > max_nodes_per_subtree:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem_ids = [str(row[0]) for row in c.execute('SELECT DISTINCT problem_id FROM orgsource;')]\n",
    "\n",
    "query='''SELECT p.program_id, program, user_id, trs.verdict FROM\n",
    "        programs p INNER JOIN orgsource o ON o.program_id = p.program_id\n",
    "        INNER JOIN test_run_summary trs ON trs.program_id = p.program_id\n",
    "        WHERE trs.verdict<>\"ALL_FAIL\" AND problem_id=?;'''\n",
    "\n",
    "data_dict = {0:{}, 1:{}}\n",
    "counts = {'correct':{'prob':0, 'user':0, 'prog':0}, 'incorrect':{'prob':0, 'user':0, 'prog':0}}\n",
    "\n",
    "TCNN_correct_classifications_counter = 0\n",
    "\n",
    "for problem_id in problem_ids:\n",
    "    for verdict in [0,1]:\n",
    "        data_dict[verdict][problem_id] = {}\n",
    "\n",
    "    for row in c.execute(query, (problem_id,)):\n",
    "        program_id, program, user_id, verdict = row\n",
    "        program = program.encode('utf-8','ignore')\n",
    "        verdict= 1 if verdict == 'ALL_PASS' else 0\n",
    "\n",
    "        if verdict == 0: \n",
    "            if program_id in TCNN_correct_classifications:\n",
    "                TCNN_correct_classifications_counter += 1\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "\n",
    "        if user_id not in data_dict[verdict][problem_id]:\n",
    "            data_dict[verdict][problem_id][user_id] = []\n",
    "        data_dict[verdict][problem_id][user_id] += [(program_id, program)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepend_line_no(prog):\n",
    "    lines=prog.split('\\n')\n",
    "    lines = ['[%2d] %s' % (idx+1, line) for idx, line in enumerate(lines)]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def remove_all_white_space(line):\n",
    "    return ''.join(line.split())\n",
    "\n",
    "def normalize_brackets(program):\n",
    "    program = program.replace('\\r', '\\n')\n",
    "    lines = [line for line in program.split('\\n') if len(line.strip()) > 0]\n",
    "    \n",
    "    if len(lines) == 1:\n",
    "        raise ValueError()\n",
    "\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        line = lines[i]\n",
    "        wsr_line = remove_all_white_space(line)\n",
    "        if wsr_line == '}' or wsr_line == '}}' or wsr_line == '}}}' or wsr_line == '};' \\\n",
    "        or wsr_line == '}}}}' or wsr_line == '}}}}}' or wsr_line == '{' or wsr_line == '{{':\n",
    "            if i > 0:\n",
    "                lines[i-1] += ' ' + line.strip()\n",
    "                lines[i]    = ''\n",
    "            else:\n",
    "                # can't handle this case!\n",
    "                raise ValueError()\n",
    "\n",
    "    # Remove empty lines\n",
    "    for i in range(len(lines)-1, -1, -1):\n",
    "        if lines[i] == '':\n",
    "            del lines[i]\n",
    "\n",
    "    for line in lines:\n",
    "        assert(lines[i].strip() != '')\n",
    "\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "def remove_empty_lines(lines):\n",
    "    return [line for line in lines if len(line.strip()) > 0]\n",
    "\n",
    "def do_diff(file1_data, file2_data, file1_name='INC', file2_name='REF'):\n",
    "    file1_data, file2_data = map(normalize_brackets, [file1_data, file2_data])\n",
    "    file1_lines = remove_empty_lines(file1_data.replace('\\r', '\\n').split('\\n'))\n",
    "    file2_lines = remove_empty_lines(file2_data.replace('\\r', '\\n').split('\\n'))\n",
    "    \n",
    "    diff_generator = difflib.unified_diff(file1_lines, file2_lines, file1_name, file2_name, n=0)    \n",
    "    output = []\n",
    "    for line in diff_generator:\n",
    "        output.append(line)\n",
    "    output = '\\n'.join(output)\n",
    "    return output\n",
    "\n",
    "def get_diff_len(diff_out):\n",
    "    diff_lines = [line for line in diff_out.split('\\n') if len(line.strip())>0 and \\\n",
    "                    not line.startswith('+++') and not line.startswith('---') and \\\n",
    "                    not line.startswith('@') and \\\n",
    "                    (line.startswith('+') or line.startswith('-'))]\n",
    "    # ignore white space diffs: having only + or - diff indicators\n",
    "    diff_lines = [line for line in diff_lines if len(line.strip())>1]\n",
    "    return len(diff_lines)\n",
    "\n",
    "def get_localization_info(diff_output):\n",
    "    diff_lines = [line for line in diff_output.split('\\n') if len(line.strip())>0 and \\\n",
    "                    not line.startswith('+++') and not line.startswith('---') and \\\n",
    "                    (line.startswith('+') or line.startswith('-') or line.startswith('@@'))]\n",
    "    \n",
    "    edit_locations = []\n",
    "    \n",
    "    for line in diff_lines:\n",
    "        if line.startswith('@@'):                \n",
    "            more = 0\n",
    "            inc_file_line_token = line.strip().split()[1]\n",
    "            if ',' in inc_file_line_token:\n",
    "                line_no, offset = map(int, inc_file_line_token.split(','))\n",
    "            else:\n",
    "                line_no, offset = int(inc_file_line_token), 1\n",
    "\n",
    "            if line_no < 0:\n",
    "                edit_locations.append(abs(line_no))\n",
    "                    \n",
    "        elif line.startswith('-'):\n",
    "            if len(line.strip()) > 1:\n",
    "                if (abs(line_no)+more) not in edit_locations:\n",
    "                    edit_locations.append(abs(line_no)+more)\n",
    "            else:\n",
    "                assert abs(line_no) > 2, abs(line_no)\n",
    "                if (abs(line_no)-1) not in edit_locations:\n",
    "                    edit_locations.append(abs(line_no)-1)\n",
    "            more += 1\n",
    "            assert more <= offset, 'more:%d, offset:%d' % (more, offset)\n",
    "\n",
    "    return edit_locations\n",
    "\n",
    "\n",
    "def get_prog(program_id, flag_prepend_line_no=True, clean_up=False):\n",
    "    query='''SELECT program FROM orgsource WHERE program_id=?;'''\n",
    "\n",
    "    with sqlite3.connect(db_path) as conn:\n",
    "        c = conn.cursor()\n",
    "        for row in c.execute(query, (program_id, )):\n",
    "            program = row[0].encode('utf-8', 'ignore')\n",
    "    \n",
    "            if clean_up:\n",
    "                program = normalize_brackets(program)\n",
    "                program_lines = remove_empty_lines(program.replace('\\r', '\\n').split('\\n'))\n",
    "                program = '\\n'.join(program_lines)\n",
    "\n",
    "            if flag_prepend_line_no:\n",
    "                return prepend_line_no(program)\n",
    "            else:\n",
    "                return program\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff with submissions from different students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inc_data_len 1449\n",
      "corr_data_len 8192\n"
     ]
    }
   ],
   "source": [
    "inc_data = {}\n",
    "corr_data = {}\n",
    "\n",
    "inc_data_len = 0\n",
    "corr_data_len = 0\n",
    "\n",
    "for problem_id in data_dict[0].keys():\n",
    "    inc_data[problem_id] = []\n",
    "    for user in data_dict[0][problem_id].keys():\n",
    "        for program_id, program in data_dict[0][problem_id][user]:\n",
    "            if program_id in eval_set_program_ids:\n",
    "                inc_data[problem_id].append( (user, program_id, program) )\n",
    "                inc_data_len += 1\n",
    "print 'inc_data_len', inc_data_len\n",
    "\n",
    "for problem_id in data_dict[1].keys():\n",
    "    corr_data[problem_id] = []\n",
    "    for user in data_dict[1][problem_id].keys():\n",
    "        for program_id, program in data_dict[1][problem_id][user]:\n",
    "            corr_data[problem_id].append( (user, program_id, program) )\n",
    "            corr_data_len += 1\n",
    "print 'corr_data_len', corr_data_len\n",
    "\n",
    "diff_lens = {}\n",
    "diff_outputs = {}\n",
    "min_diff = {}\n",
    "\n",
    "for problem_id in inc_data.keys():\n",
    "    diff_outputs[problem_id] = {}\n",
    "    for (i_user, i_program_id, i_program), (c_user, c_program_id, c_program) in \\\n",
    "    itertools.product(inc_data[problem_id], corr_data[problem_id]):\n",
    "        if i_user != c_user:\n",
    "            diff_out = do_diff(i_program, c_program, i_program_id, c_program_id)\n",
    "            if user not in diff_outputs[problem_id]:\n",
    "                diff_outputs[problem_id][user] = []\n",
    "            diff_outputs[problem_id][user] += [(i_program, c_program, diff_out)]\n",
    "            diff_len = get_diff_len(diff_out)\n",
    "            if i_program_id in diff_lens:\n",
    "                if diff_len < diff_lens[i_program_id]:\n",
    "                    diff_lens[i_program_id] = diff_len\n",
    "                    min_diff[i_program_id] = (c_program_id, diff_out, diff_len)\n",
    "            else:\n",
    "                diff_lens[i_program_id] = diff_len\n",
    "                min_diff[i_program_id] = (c_program_id, diff_out, diff_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def localize_bugs(buggy_lines, predictions):\n",
    "    set_buggy_lines = set(buggy_lines)\n",
    "    set_predictions = set(predictions)\n",
    "    lines_found = len(set_buggy_lines & set_predictions)\n",
    "    return lines_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_lines: 2496 total_programs: 1449 \n",
      "\n",
      "k: 1 | lines_found:   0 ( 0.00%) | programs_found:   0 ( 0.00%)\n",
      "k: 5 | lines_found: 136 ( 5.45%) | programs_found: 116 ( 8.01%)\n",
      "k:10 | lines_found: 801 (32.09%) | programs_found: 600 (41.41%)\n"
     ]
    }
   ],
   "source": [
    "total_programs_found, total_programs = {1:0,5:0,10:0}, 0\n",
    "total_lines_found, total_lines = {1:0,5:0,10:0}, 0\n",
    "\n",
    "top_k = [1,5,10]\n",
    "\n",
    "for i_program_id in min_diff:\n",
    "    c_program_id, diff_out, diff_len = min_diff[i_program_id]\n",
    "    edit_locations = get_localization_info(diff_out)\n",
    "    lines_found = localize_bugs(prog_faulty_lines[i_program_id], edit_locations)\n",
    "    if lines_found > 0:\n",
    "        for k in top_k:\n",
    "            if len(edit_locations) <= k:\n",
    "                total_programs_found[k] += 1\n",
    "                total_lines_found[k] += (lines_found)\n",
    "    total_programs += 1\n",
    "    total_lines += len(eval_dict[i_program_id][3])\n",
    "    \n",
    "print 'total_lines:', total_lines, 'total_programs:', total_programs, '\\n'\n",
    "    \n",
    "for k in top_k:\n",
    "    print 'k:%2d |' % k,\n",
    "    print 'lines_found: %3d (%5.2f%%) |' % (total_lines_found[k], (100.0*total_lines_found[k]/total_lines)),\n",
    "    print 'programs_found: %3d (%5.2f%%)' % (total_programs_found[k], (100.0*total_programs_found[k]/total_programs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cpu-tf-1.0.1]",
   "language": "python",
   "name": "conda-env-cpu-tf-1.0.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
